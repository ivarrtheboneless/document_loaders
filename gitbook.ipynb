{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb438c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from urllib.parse import urlparse\n",
    "from langchain.document_loaders import GitbookLoader as OriginalGitbookLoader\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, page_content: str, metadata: Dict[str, str]):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "class GitbookLoader(OriginalGitbookLoader):\n",
    "    def __init__(self, web_page, load_all_paths=False, base_url=''):\n",
    "        # Call the parent constructor with the same arguments\n",
    "        super().__init__(web_page, load_all_paths, base_url)\n",
    "\n",
    "        # Check if the web_page attribute ends with a slash and update self.base_url accordingly\n",
    "        if self.base_url is None and self.web_page.endswith('/'):\n",
    "            self.base_url = self.web_page[:-1]\n",
    "\n",
    "    def remove_prefix(self, prefix: str, lst: List[str]) -> List[str]:\n",
    "        \"\"\"Remove prefix from all elements in lst.\"\"\"\n",
    "        return [s[len(prefix):] if s.startswith(prefix) else s for s in lst]\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Fetch text from one single GitBook page.\"\"\"\n",
    "        if self.load_all_paths:\n",
    "            soup_info = self.scrape()\n",
    "            relative_paths = self._get_paths(soup_info)\n",
    "            # Extract the prefix from the given URL\n",
    "            url_path = urlparse(self.base_url).path\n",
    "            prefix_to_remove = url_path.split(\"/\", 2)[1]  # Get the first part of the path\n",
    "            # Remove the dynamic prefix from the relative paths\n",
    "            relative_paths = self.remove_prefix(f\"/{prefix_to_remove}\", relative_paths)            \n",
    "            documents = []\n",
    "            for path in relative_paths:\n",
    "                url = self.base_url + path\n",
    "                print(f\"Fetching text from {url}\")\n",
    "                soup_info = self._scrape(url)\n",
    "                documents.append(self._get_document(soup_info, url))\n",
    "            return documents\n",
    "        else:\n",
    "            soup_info = self.scrape()\n",
    "            return [self._get_document(soup_info, self.web_path)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96a520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
